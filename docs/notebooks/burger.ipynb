{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *checkpoint_schedule* application: Adjoint-Based Gradient with the Burger's Equation\n",
    "\n",
    "This example shows adjoint-based gradient computation using the *checkpointing_schedules* package. We initially define the adjoint-based gradient problem and then present the forward and adjoint solvers prescribed by the *checkpointing_schedules* package.\n",
    "\n",
    "## Defining the application\n",
    "\n",
    "Let us consider a one-dimensional (1D) problem aiming to compute the gradient/sensitivity of the kinetic energy at a time $t=\\tau$ with respect to an initial condition $u_0$. Then, our functional can be expressed as:\n",
    "$$\n",
    "I(u(x, \\tau, u_0)) = \\int_\\Omega \\frac{1}{2} u(x, \\tau, u_0)u(x, \\tau, u_0) \\, dx.\n",
    "\\tag{1} \n",
    "$$\n",
    "\n",
    "In the current case, the velocity variable $u = u(x, t)$ is governed by the 1D viscous Burgers equation, a non-linear equation for the advection and diffusion of momentum:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} - \\nu \\frac{\\partial^2 u}{\\partial x^2} = 0.\n",
    "\\tag{2}\n",
    "$$\n",
    "\n",
    "Here, $x \\in [0, L]$ is the space variable, and $t \\in \\mathbb{R}^{+}$ represents the time variable. The boundary condition is $u(0, t) = u(L, t) = 0$, where $L$ is the length of the 1D domain. The initial condition is given by $u_0 = \\sin(\\pi x)$.\n",
    "\n",
    "The control parameter is the initial condition $u_0$. Hence, the objective is to compute the adjoint-based gradient of the functional $I(u_0)$ with respect to $u_0$.\n",
    "\n",
    "This example uses the discrete adjoint formulation, meaning the adjoint system is from the discrete forward PDE system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjoint-based gradient\n",
    "We wain to compute the sensitivity/gradient $\\mathrm{d}\\widehat{I}/\\mathrm{d}u_0$, where $\\widehat{I} = I(u_0)$ is the reduced functional. On applying the chain rule, we have the below expression on considering the functional $I$ as in Eq. (1).\n",
    "$$\\frac{\\mathrm{d}\\widehat{I}}{\\mathrm{d}u_0} = \\frac{\\partial I}{\\partial u} \\frac{\\mathrm{d}u}{\\mathrm{d}u_0}. \\tag{3}$$\n",
    "Computing $\\partial J/\\partial u$ is straightforward since the functional (Eq. (1)) is written in terms of the solution variable $u$. In contrast, the Jacobian $\\mathrm{d}u/\\mathrm{d}u_0$ involves an expensive computational operation that is avoided by using the adjoint method.\n",
    "\n",
    "In summary, the adjoint system is defined by using the relation of the functional $I$ with the forward equation $F(u, u_0)$. Taking the total derivative of $F(u, u_0)$ with respect to $u_0$ yields a relationship for the solution Jacobian $\\mathrm{d}u/\\mathrm{d}u_0$:\n",
    "$$\\frac{\\mathrm{d}}{\\mathrm{d}u_0} F(u, u_0) = \\frac{\\partial F(u, u_0)}{\\partial u} \\frac{\\mathrm{d}u}{\\mathrm{d}u_0} + \\frac{\\partial F(u, u_0)}{\\partial u_0} = 0 \\\\\n",
    " \\implies \\frac{\\mathrm{d}u}{\\mathrm{d}u_0} = - \\left(\\frac{\\partial F(u, u_0)}{\\partial u}\\right)^{-1} \\frac{\\partial F(u, u_0)}{\\partial u_0}.$$\n",
    "Substituting $\\mathrm{d}u/\\mathrm{d}u_0$ into Eq. (3), we have:\n",
    "$$\\frac{\\mathrm{d}\\widehat{I}}{\\mathrm{d}u_0} = - \\frac{\\partial I}{\\partial u} \\left(\\frac{\\partial F(u, u_0)}{\\partial u}\\right)^{-1} \\frac{\\partial F(u, u_0)}{\\partial u_0}.$$\n",
    "Taking the adjoint (Hermitian transpose) of the above equation:\n",
    "$$\\frac{\\mathrm{d}\\widehat{I}}{\\mathrm{d}u_0}^* = - \\frac{\\partial F}{\\partial u_0}^* \\frac{\\partial F}{\\partial u}^{-*} \\frac{\\partial I}{\\partial u}^{*}$$\n",
    "\n",
    "Next, we define:\n",
    "$$\\lambda = \\left(\\frac{\\partial F(u, u_0)}{\\partial u}\\right)^{-*} \\frac{\\partial I}{\\partial u}^* \n",
    "  \\implies  \\left(\\frac{\\partial F(u, u_0)}{\\partial u}\\right)^{*} \\lambda = \\frac{\\partial I}{\\partial u}^*,$$\n",
    "which is the adjoint system. Finally, we obtain the following expression for the adjoint-based gradient computation:\n",
    "$$\\frac{\\mathrm{d}\\widehat{I}}{\\mathrm{d} u_0}^* = - \\frac{\\partial F}{\\partial u_0}^* \\lambda.$$\n",
    "In this current case, Burger's equation is written in weak form and discretized in time with the backward finite difference method, we have:\n",
    "$$ F(u^{n+1}, u^n, v) = \\int_{\\Omega} u^{n + 1} \\cdot v - u^{n}  \\cdot v + \\Delta t \\left(u^{n + 1} \\frac{\\partial u^{n + 1}}{\\partial x} \\cdot v + \\frac{\\partial u^{n + 1}}{\\partial x}  \\cdot \\frac{\\partial v}{\\partial x}\\right) \\, dx = 0 \\quad \\forall u, v\\in V \\tag{4},$$\n",
    "where $v \\in V$ is an arbitrary test function. The Jacobian of $F(u^{n+1}, u^n, v)$ is obtained through the Gateaux derivative. \n",
    "\n",
    "The time sequence of the adjoint system for the functional (Eq. (1)) is given by:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\lambda^{N+1} &= \\frac{\\partial I}{\\partial u^{N + 1}} \\\\\n",
    "    \\\\\n",
    "    \\frac{\\partial F(u^{N}, u^{N + 1})}{\\partial u^{N}} \\lambda^{N} &= - \\frac{\\partial F(u^{N}, u^{N + 1})}{\\partial u^{N + 1}} \\lambda^{N + 1} \\\\\n",
    "    \\\\\n",
    "    \\frac{\\partial F(u^{N-1}, u^{N})}{\\partial u^{N-1}} \\lambda^{N-1} &= - \\frac{\\partial F(u^{N-1}, u^{N})}{\\partial u^{N}} \\lambda^{N} \\\\\n",
    "    \\\\\n",
    "    \\vdots \\\\\n",
    "    \\\\\n",
    "    \\frac{\\partial F(u^{1}, u^{2})}{\\partial u^{1}} \\lambda^{1} &= - \\frac{\\partial F(u^{1}, u^{2})}{\\partial u^{2}} \\lambda^{2} \\\\\n",
    "    \\\\\n",
    "    \\lambda^{0} &= - \\frac{\\partial F(u^{0}, u^{1})}{\\partial u^{1}} \\lambda^{1}.\n",
    "\\end{align*}\n",
    "\\tag{5}\n",
    "$$\n",
    "where $\\lambda^{n}$ is the adjoint variable at time step $n$.\n",
    "\n",
    "The adjoint system is solved backward in time, we can verify this by taking the adjoint of Eq. (5), where at the time step zero we have two unknowns, $\\lambda^{0}$ and $\\lambda^{1}$, and at the time step $N$ we have only one unknown, $\\lambda^{N}$.\n",
    "\n",
    "For additional details on the discrete adjoint-based gradient computation, you may refer to the following references [1, 2].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Burger's equation solvers\n",
    "We define a `BurgersEquation` class to compute the `forward` and `adjoint` solvers. In addition, the `BurgersEquation` class defines a `copy_data` method that copies the data from one storage type to another, and an `adjoint_initial_condition` method that initializes the adjoint.\n",
    "\n",
    "The Burger's equation is discretised using the Finite Element Method (FEM). We use continuous first-order Lagrange basis functions to discretise the spatial domain.\n",
    "\n",
    "After discretising, the residual of the forward system is given by:\n",
    "$$\\mathbf{F} = \\mathbf{M} \\mathbf{U}^{n + 1} - \\mathbf{M} \\mathbf{U}^{n} + \\Delta t \\left(\\mathbf{C}(\\mathbf{U}^{n + 1}) \\mathbf{U}^{n + 1} - \\mathbf{K} \\mathbf{U}^{n + 1}\\right) = 0, \\tag{6}$$\n",
    "where $M$ and $K$ are the mass and stiffness, respectively. $U$ represents the solution vector. The matrices $C$ is obtained from the advection term. The superscript $n$ denotes the time step, and $\\Delta t$ is the time step size. The solution vector $\\mathbf{U}^{n + 1}$ is obtained by solving the non-linear forward system using the Newton method. The Jacobian of the forward system has the form:\n",
    "\n",
    "Let us consider the residual of the forward system as $\\mathbf{R}(\\mathbf{U}^{n + 1}, \\mathbf{U}^{n})$. The Jacobian of the forward system is given by:\n",
    "$$\\mathbf{J}_{n, n} = \\frac{\\partial \\mathbf{F}^{n}}{\\partial U^{n}} = \\mathbf{M} + \\Delta t \\left(\\mathbf{J_C}(\\mathbf{U}^{n}) - \\mathbf{K} \\right), \\tag{7}$$\n",
    "$$\\mathbf{J_{n + 1, n}} = \\frac{\\partial \\mathbf{F}^{n + 1}}{\\partial \\mathbf{U}^{n}} = - \\mathbf{M}, \\tag{8}$$\n",
    "where $\\mathbf{J_C}$ is the Jacobian of the advection term. Considering the adjoint as shownd in Eq. (5), and the Jacobian of the forward system (Eq. (7) a (8)), we arrive at the following expression for the adjoint system.\n",
    "$$\\mathbf{J}^{T}_{n, n} \\Lambda^{n} + \\mathbf{J}^{T}_{n+1, n} \\boldsymbol{\\Lambda}^{n + 1} = 0, \\tag{9}$$\n",
    "for $n = N - 1, \\ldots, 0$. $\\Lambda$ is the adjoint solution vector. \n",
    "\n",
    "At the time $n = N$, the adjoint is initialized by solving the following system:\n",
    "$$\\mathbf{J}^{T}_{N,N} \\boldsymbol{\\Lambda}^{N} + \\frac{\\partial \\mathbf{I}}{\\partial \\mathbf{U}^{N}} = 0. \\tag{10}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy.sparse import lil_matrix\n",
    "from scipy.optimize import newton\n",
    "import copy\n",
    "from sympy import diff, integrate, symbols, lambdify\n",
    "from sympy.matrices import SparseMatrix, Matrix\n",
    "\n",
    "\n",
    "class BurgersEquation:\n",
    "    \"\"\"A solver for the forward and adjoint one dimensional Burger's equation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : dict\n",
    "        The model parameters containing the essential information to solve\n",
    "        the Burger's equation.\n",
    "    init_condition : array\n",
    "        The initial condition used to solve the forward Burger's equation.\n",
    "    mesh : array\n",
    "        The spatial mesh.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, forward_initial_condition, mesh):\n",
    "        self.model = model\n",
    "        self.mesh = mesh\n",
    "        self.forward_work_memory = {StorageType.WORK: {}}\n",
    "        self.forward_work_memory[StorageType.WORK][0] = copy.deepcopy(forward_initial_condition)\n",
    "        self.forward_final_solution = None\n",
    "        self.initial_condition = copy.deepcopy(forward_initial_condition)\n",
    "        self.adjoint_work_memory = {StorageType.WORK: {}}\n",
    "        self.restart_forward = {StorageType.RAM: {}, StorageType.DISK: {}}\n",
    "        self.adjoint_dependency = {StorageType.WORK: {}, StorageType.RAM: {}, StorageType.DISK: {}}\n",
    "        self.mode = \"forward\"\n",
    "        self._initialize_matrices()\n",
    "    \n",
    "    def forward(self, n0, n1, storage=None, write_adj_deps=False, write_ics=False):\n",
    "        \"\"\"Advance the forward solver.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n0 : int\n",
    "            Initial time step.\n",
    "        n1 : int\n",
    "            Final time step.\n",
    "        storage : StorageType, optional\n",
    "            The storage type, which can be StorageType.RAM, StorageType.DISK,\n",
    "            StorageType.WORK, or StorageType.NONE.\n",
    "        write_adj_deps : bool, optional\n",
    "            Whether the adjoint dependency data will be stored.\n",
    "        write_ics : bool, optional\n",
    "            Whether the forward restart data will be stored.\n",
    "        \"\"\"\n",
    "        # Get the initial condition\n",
    "        u = self.forward_work_memory[StorageType.WORK].pop(n0)\n",
    "        if write_ics:\n",
    "            self._store_data(u, n0, storage, write_adj_deps, write_ics)\n",
    "        for step in range(n0, min(n1, self.model[\"max_n\"])):\n",
    "            if write_adj_deps:\n",
    "                self._store_data(u, step, storage, write_adj_deps, write_ics)\n",
    "            def _residual(u_new):\n",
    "                C = lil_matrix(self._C(*u_new))\n",
    "                residual = (self._M + self.model[\"dt\"] * (self._K + C)).dot(u_new) - self._M.dot(u)\n",
    "                residual[0] = residual[self.model[\"nx\"] - 1] = 0\n",
    "                return residual\n",
    "            u_new = self._solve_newton(u, _residual)\n",
    "            u = copy.deepcopy(u_new)\n",
    "        step += 1\n",
    "        if step == self.model[\"max_n\"]:\n",
    "            self.forward_final_solution = copy.deepcopy(u_new)\n",
    "            self.adjoint_dependency[StorageType.WORK][step] = copy.deepcopy(u_new)\n",
    "        if (not write_adj_deps \n",
    "           or (self.mode == \"forward\" and step < (self.model[\"max_n\"]))\n",
    "        ):\n",
    "            self.forward_work_memory[StorageType.WORK][step] = copy.deepcopy(u_new)\n",
    "\n",
    "    def adjoint(self, n0, n1, clear_adj_deps):\n",
    "        \"\"\"Advance the adjoint solver.\n",
    "\n",
    "        Parameters\n",
    "        ---------\n",
    "        n0 : int\n",
    "            Initial time step.\n",
    "        n1 : int\n",
    "            Final time step.\n",
    "        clear_adj_deps : bool\n",
    "            If `True`, the adjoint dependency data will be cleared.\n",
    "        \"\"\"\n",
    "        self.mode = \"adjoint\"\n",
    "        if n1 <= self.model[\"max_n\"]:\n",
    "            if n1 == self.model[\"max_n\"]:\n",
    "                self._initialize_adjoint()\n",
    "            u_adj = self.adjoint_work_memory[StorageType.WORK].pop(n1)\n",
    "        step = n1\n",
    "        for n in range(n1, n0, - 1):\n",
    "            J_T = self._jacobian(self.adjoint_dependency[StorageType.WORK][n], n, n).T\n",
    "            u_adj_new = spsolve(J_T, u_adj)\n",
    "            u_adj = copy.deepcopy(u_adj_new)\n",
    "            if clear_adj_deps:\n",
    "                del self.adjoint_dependency[StorageType.WORK][n]\n",
    "        step = n - 1\n",
    "        self.adjoint_work_memory[StorageType.WORK][step] = copy.deepcopy(u_adj)\n",
    "\n",
    "    def gradient(self):\n",
    "        \"\"\"Compute the adjoint-based gradient.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array\n",
    "            The gradient.\n",
    "        \"\"\"\n",
    "        J = self._jacobian(self.initial_condition, 1, 0)\n",
    "        grad = J.T.dot(self.adjoint_work_memory[StorageType.WORK][0])\n",
    "        grad[0] = grad[self.model[\"nx\"] - 1] = 0\n",
    "        return grad\n",
    "    \n",
    "    def functional(self):\n",
    "        \"\"\"Compute the functional.\n",
    "        \"\"\"\n",
    "        return 0.5 * self.forward_final_solution.dot(self.forward_final_solution)\n",
    "    \n",
    "    def copy_data(self, step, from_storage, to_storage, move=False):\n",
    "        \"\"\"Copy data from one storage to another.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        step : int\n",
    "            The time step.\n",
    "        from_storage : StorageType\n",
    "            The storage type from which the data will be copied.\n",
    "        to_storage : StorageType\n",
    "            The storage type to which the data will be copied.\n",
    "        move : bool, optional\n",
    "            Whether the data will be moved or not. If `True`, the data will be\n",
    "            removed from the `from_storage`.\n",
    "        \"\"\"\n",
    "        if from_storage == StorageType.DISK:\n",
    "            if step in self.adjoint_dependency[StorageType.DISK]:\n",
    "                file_name = self.adjoint_dependency[StorageType.DISK][step]\n",
    "                with open(file_name, \"rb\") as f:\n",
    "                    self.adjoint_dependency[to_storage][step] = np.load(f)\n",
    "            if step in self.restart_forward[StorageType.DISK]:\n",
    "                file_name = self.restart_forward[StorageType.DISK][step]\n",
    "                with open(file_name, \"rb\") as f:\n",
    "                    self.forward_work_memory[to_storage][step] = np.load(f)\n",
    "            if move:\n",
    "                os.remove(file_name)\n",
    "        elif from_storage == StorageType.RAM:\n",
    "            self.forward_work_memory[to_storage][step] = copy.deepcopy(self.restart_forward[from_storage][step])\n",
    "            if move:\n",
    "                if step in self.adjoint_dependency[from_storage]:\n",
    "                    del self.adjoint_dependency[from_storage][step]\n",
    "                if step in self.restart_forward[from_storage]:\n",
    "                    del self.restart_forward[from_storage][step]\n",
    "\n",
    "    def _initialize_adjoint(self):\n",
    "        J = self._jacobian(self.forward_final_solution, self.model[\"max_n\"], self.model[\"max_n\"]).T\n",
    "        M = copy.deepcopy(self._M).T\n",
    "        u_adj = self.forward_final_solution\n",
    "        self.adjoint_work_memory[StorageType.WORK][self.model[\"max_n\"]] = u_adj\n",
    "\n",
    "    def _initialize_matrices(self):\n",
    "        # Initialize the mass and stiffness matrices.\n",
    "        self._M = self._mass_matrix()\n",
    "        self._K = self._stiffness_matrix()\n",
    "        u = symbols(\"u_:{0}\".format(len(self.initial_condition)))\n",
    "        # Initialize the advection matrix.\n",
    "        self._C = lambdify(u, self._advection_matrix(u), 'numpy')\n",
    "        # Initialize the Jacobian of the advection matrix.\n",
    "        self._J_C = lambdify(u, self._jacobian_advection(u), 'numpy')\n",
    "\n",
    "    def _jacobian(self, u, n, m):\n",
    "        if n == m:\n",
    "            C_J = lil_matrix(self._J_C(*u))\n",
    "            Jac = self._M + self.model[\"dt\"] * (self._K + C_J)\n",
    "        elif m == n - 1:\n",
    "            Jac = self._M\n",
    "        Jac[0, :] = Jac[-1, :] = 0\n",
    "        Jac[0, 0] = Jac[-1, -1] = 1\n",
    "        return Jac\n",
    "    \n",
    "    def _solve_newton(self, u_prev, residual, tol=1e-8, max_iter=50):\n",
    "        i = 0\n",
    "        u_prev[0] = u_prev[-1] = 0\n",
    "        u = copy.deepcopy(u_prev)\n",
    "        while i < max_iter:\n",
    "            Jac = self._jacobian(u, i, i)\n",
    "            delta_u = spsolve(Jac, residual(u))\n",
    "            u_new = u - delta_u\n",
    "            u = copy.deepcopy(u_new)\n",
    "            i += 1\n",
    "            if np.linalg.norm(delta_u) < tol:\n",
    "                break\n",
    "        if i == max_iter:\n",
    "            print(\"Newton's method did not converge in {} iterations.\".format(max_iter))\n",
    "        return u_new\n",
    "    \n",
    "    def _basis_function(self):\n",
    "        eta = symbols(\"eta\")\n",
    "        # eta = (x - x0) / h.\n",
    "        h = self.model[\"lx\"] / (self.model[\"nx\"] - 1)\n",
    "        return [1 - eta, eta]\n",
    "    \n",
    "    def _mass_matrix(self):\n",
    "        num_nodes = self.model[\"nx\"]\n",
    "        h = self.model[\"lx\"] / (self.model[\"nx\"] - 1) \n",
    "        phi = self._basis_function()\n",
    "        M_local = np.zeros((2, 2))\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                # dx = deta * h. Thus, M_local = h * integrate(phi[i] * phi[j], (\"eta\", 0, 1))\n",
    "                M_local[i, j] = h * integrate(phi[i] * phi[j], (\"eta\", 0, 1))\n",
    "        M_global = lil_matrix((num_nodes, num_nodes))\n",
    "        M_global[0, 0] = M_global[num_nodes - 1, num_nodes - 1] = M_local[0, 0]\n",
    "        M_global[0, 1] = M_global[num_nodes - 1, num_nodes - 2] = M_local[0, 1]\n",
    "        for i in range(1, num_nodes - 1):\n",
    "            M_global[i, i - 1] = M_global[i, i + 1] = M_local[1, 0]\n",
    "            M_global[i, i] = M_local[1, 1] + M_local[0, 0]\n",
    "        return M_global\n",
    "    \n",
    "    def _stiffness_matrix(self):\n",
    "        num_nodes = self.model[\"nx\"]\n",
    "        # 1D mesh is uniform. Thus, the mesh spacing is constant.\n",
    "        h = self.model[\"lx\"] / (self.model[\"nx\"] - 1) \n",
    "        b = self.model[\"nu\"] / h\n",
    "        phi = self._basis_function()\n",
    "        dphi_deta = [diff(phi[i], \"eta\") for i in range(2)]\n",
    "        K_local = np.zeros((2, 2))\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                K_local[i, j] = b * integrate(dphi_deta[i] * dphi_deta[j], (\"eta\", 0, 1))\n",
    "        K = lil_matrix((num_nodes, num_nodes))\n",
    "        K[0, 0] = K_local[0, 0]\n",
    "        K[0, 1] = K_local[0, 1]\n",
    "        K[num_nodes - 1, num_nodes - 2] = K_local[1, 0]\n",
    "        K[num_nodes - 1, num_nodes - 1] = K_local[1, 1]\n",
    "        for i in range(1, num_nodes - 1):\n",
    "            K[i, i - 1] = K_local[1, 0]\n",
    "            K[i, i] = K_local[1, 1] + K_local[0, 0]\n",
    "            K[i, i + 1] = K_local[0, 1]\n",
    "        return K\n",
    "    \n",
    "    def _advection_matrix(self, u):\n",
    "        num_nodes = self.model[\"nx\"]\n",
    "        # 1D mesh is uniform. Thus, the mesh spacing is constant.\n",
    "        u0, u1, eta = symbols(\"u0 u1 eta\")\n",
    "        phi = Matrix(self._basis_function())\n",
    "        u_nodes_local = Matrix([u0, u1])\n",
    "        # Create a symbolic vector\n",
    "        symbolic_vector = Matrix(u)\n",
    "        dphi_deta = phi.diff(eta)\n",
    "        u_ = phi[0] * u0 + phi[1] * u1\n",
    "        C = SparseMatrix.zeros(num_nodes, num_nodes)\n",
    "        nonlinear_term = Matrix([u_ * dphi_deta[j] * phi[i] for i in range(2) for j in range(2)])\n",
    "        local_matrix = Matrix([integrate(nonlinear_term[i], (\"eta\", 0, 1)) for i in range(4)])\n",
    "        local_matrix_func = lambdify((u0, u1), local_matrix, 'numpy')\n",
    "\n",
    "        def local_evaluator(u0, u1):\n",
    "            return np.reshape(local_matrix_func(u0, u1), (2, 2))\n",
    "        local_evaluator = np.vectorize(local_evaluator)\n",
    "        C[0, 0] = local_evaluator(u[0], u[1])[0, 0]\n",
    "        C[0, 1] = local_evaluator(u[0], u[1])[0, 1]\n",
    "        C[num_nodes - 1, num_nodes - 2] = local_evaluator(u[num_nodes - 2], u[num_nodes - 1])[1, 0]\n",
    "        C[num_nodes - 1, num_nodes - 1] = local_evaluator(u[num_nodes - 2], u[num_nodes - 1])[1, 1]\n",
    "        for i in range(1, num_nodes - 1):\n",
    "            C[i, i - 1] = local_evaluator(u[i - 1], u[i])[1, 0]\n",
    "            C[i, i] = local_evaluator(u[i - 1], u[i])[1, 1] + local_evaluator(u[i], u[i + 1])[0, 0]\n",
    "            C[i, i + 1] = local_evaluator(u[i], u[i + 1])[0, 1]\n",
    "        return C\n",
    "    \n",
    "    def _jacobian_advection(self, u):\n",
    "        N = self.model[\"nx\"]\n",
    "        u0, u1, eta = symbols(\"u0 u1 eta\")\n",
    "        phi = Matrix(self._basis_function())\n",
    "        u_nodes_local = Matrix([u0, u1])\n",
    "        # Create a symbolic vector\n",
    "        symbolic_vector = Matrix(u)\n",
    "        dphi_deta = phi.diff(eta)\n",
    "        u_ = phi[0] * u0 + phi[1] * u1\n",
    "        Jc = SparseMatrix.zeros(N, N)\n",
    "        term_0 = Matrix([phi[j] * u_nodes_local.dot(dphi_deta) * phi[i]\n",
    "                         for i in range(2) for j in range(2)])\n",
    "        term_1 = Matrix([u_ * dphi_deta[j] * phi[i] for i in range(2) for j in range(2)])\n",
    "        local_matrix = Matrix([integrate(term_0[i] + term_1[i], (\"eta\", 0, 1)) for i in range(4)])\n",
    "        local_matrix_func = lambdify((u0, u1), local_matrix, 'numpy')\n",
    "\n",
    "        def local_evaluator(u0, u1):\n",
    "            return np.reshape(local_matrix_func(u0, u1), (2, 2))\n",
    "        local_evaluator = np.vectorize(local_evaluator)\n",
    "        Jc[0, 0] = local_evaluator(u[0], u[1])[0, 0]\n",
    "        Jc[0, 1] = local_evaluator(u[0], u[1])[0, 1]\n",
    "        Jc[N - 1, N - 2] = local_evaluator(u[N - 2], u[N - 1])[1, 0]\n",
    "        Jc[N - 1, N - 1] = local_evaluator(u[N - 2], u[N - 1])[1, 1]\n",
    "        for i in range(1, N - 1):\n",
    "            Jc[i, i - 1] = local_evaluator(u[i - 1], u[i])[1, 0]\n",
    "            Jc[i, i] = local_evaluator(u[i - 1], u[i])[1, 1] + local_evaluator(u[i], u[i + 1])[0, 0]\n",
    "            Jc[i, i + 1] = local_evaluator(u[i], u[i + 1])[0, 1]\n",
    "        return Jc\n",
    "       \n",
    "    \n",
    "    def _store_data(self, data, step, storage, write_adj_deps, write_ics):\n",
    "        if storage == StorageType.DISK:\n",
    "            self._store_on_disk(data, step, write_adj_deps)\n",
    "        elif storage == StorageType.RAM:\n",
    "            if write_adj_deps:\n",
    "                self.adjoint_dependency[StorageType.RAM][step] = copy.deepcopy(data)\n",
    "            if write_ics:\n",
    "                self.restart_forward[StorageType.RAM][step] = copy.deepcopy(data)\n",
    "        else:\n",
    "            if write_adj_deps:\n",
    "                self.adjoint_dependency[StorageType.WORK][step] = copy.deepcopy(data)\n",
    "            if write_ics:\n",
    "                self.restart_forward[StorageType.WORK][step] = copy.deepcopy(data)\n",
    "\n",
    "    def _store_on_disk(self, data, step, adj_deps):\n",
    "        if adj_deps:\n",
    "            file_name = \"adj_dependency/fwd_\"+ str(step) +\".npy\"\n",
    "            self.adjoint_dependency[StorageType.DISK][step] = file_name\n",
    "            with open(file_name, \"wb\") as f:\n",
    "                np.save(f, data)\n",
    "        else:\n",
    "            file_name = \"restart_forward/fwd_\"+ str(step) +\".npy\"\n",
    "            with open(file_name, \"wb\") as f:\n",
    "                np.save(f, data)\n",
    "            self.restart_forward[StorageType.DISK][step] = file_name\n",
    "    \n",
    "    def _clean_disk(self):\n",
    "        if len(self.adjoint_dependency[StorageType.DISK]) > 0:\n",
    "            for step in self.adjoint_dependency[StorageType.DISK]:\n",
    "                os.remove(self.adjoint_dependency[StorageType.DISK][step])\n",
    "        if len(self.restart_forward[StorageType.DISK]) > 0:\n",
    "            for step in self.restart_forward[StorageType.DISK]:\n",
    "                os.remove(self.restart_forward[StorageType.DISK][step])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpointing\n",
    "The adjoint PDE is then solved in a reverse time order and it depends of the forward data. Storing the entire forward solution uses storage linear linear in the number of time steps, which can exhaust the available storage. To overcome this kind of problem, checkpointing algorithms are used to reduce the memory usage.\n",
    "\n",
    "To employ a checkpointing method in the adjoint-based sensitivity calculation, we define a `CheckpointingManager` to manage the execution of forward and adjoint models. The `CheckpointingManager` defines the `execute` method, which performs each action specified in a provided checkpoint schedule (`_schedule`). Within the `execute` method, we have the single-dispatch generic `action` function that is overloaded by particular functions. For instance, if the `checkpoint_schedule` action is `Forward`, the `action_forward` function is called, and this can advance the forward calculation. In this particular example, the forward solver is executed by calling `self.equation.forward`. Here, `self.equation` is an attribute of `CheckpointingManager`. Similarly, the adjoint solver is executed by calling `self.equation.adjoint` within the `action_reverse` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools, sys\n",
    "from checkpoint_schedules import *\n",
    "class CheckpointingManager:\n",
    "    \"\"\"Manage the forward and adjoint solvers.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    schedule : CheckpointSchedule\n",
    "        The schedule created by `checkpoint_schedules` package.\n",
    "    equation : object\n",
    "        An equation object used to solve the forward and adjoint solvers.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    The `equation` object contains methods to execute the forward and adjoint. In \n",
    "    addition, it contains methods to copy data from one storage to another, and\n",
    "    to set the initial condition for the adjoint.\n",
    "    \"\"\"\n",
    "    def __init__(self, schedule, equation):\n",
    "        self.max_n = sys.maxsize\n",
    "        self.equation = equation\n",
    "        self.reverse_step = 0\n",
    "        self._schedule = schedule\n",
    "        \n",
    "    def execute(self):\n",
    "        \"\"\"Execute forward and adjoint using checkpointing.\n",
    "        \"\"\"\n",
    "        @functools.singledispatch\n",
    "        def action(cp_action):\n",
    "            raise TypeError(\"Unexpected action\")\n",
    "\n",
    "        @action.register(Forward)\n",
    "        def action_forward(cp_action):\n",
    "            n1 = cp_action.n1\n",
    "            self.equation.forward(cp_action.n0, n1, storage=cp_action.storage,\n",
    "                                  write_adj_deps=cp_action.write_adj_deps,\n",
    "                                  write_ics=cp_action.write_ics)\n",
    "            if n1 >= self.equation.model[\"max_n\"]:\n",
    "                n1 = min(n1, self.equation.model[\"max_n\"])\n",
    "                self._schedule.finalize(n1)\n",
    "\n",
    "        @action.register(Reverse)\n",
    "        def action_reverse(cp_action):\n",
    "            self.equation.adjoint(cp_action.n0, cp_action.n1, cp_action.clear_adj_deps)\n",
    "            self.reverse_step += cp_action.n1 - cp_action.n0\n",
    "            \n",
    "        @action.register(Copy)\n",
    "        def action_copy(cp_action):\n",
    "            self.equation.copy_data(cp_action.n, cp_action.from_storage, cp_action.to_storage, move=False)\n",
    "\n",
    "        @action.register(Move)\n",
    "        def action_move(cp_action):\n",
    "            self.equation.copy_data(cp_action.n, cp_action.from_storage, cp_action.to_storage, move=True)\n",
    "            \n",
    "        @action.register(EndForward)\n",
    "        def action_end_forward(cp_action):\n",
    "            if self._schedule.max_n is None:\n",
    "                self._schedule._max_n = self.max_n\n",
    "            \n",
    "        @action.register(EndReverse)\n",
    "        def action_end_reverse(cp_action):\n",
    "            self.equation._clean_disk()\n",
    "            if self._schedule.max_n != self.reverse_step:\n",
    "                raise ValueError(\"The number of steps in the reverse phase\"\n",
    "                                 \"is different from the number of steps in the\"\n",
    "                                 \"forward phase.\")\n",
    "            \n",
    "        self.reverse_step = 0\n",
    "        for _, cp_action in enumerate(self._schedule):\n",
    "            action(cp_action)\n",
    "            if isinstance(cp_action, EndReverse):\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjoint-based sensitivity computations\n",
    "\n",
    "The purpose of this adjoint-based sensitivity computation is to use every checkpointing approach available in the `checkpoint_schedules` package and verify the consistent of the results. We employ a fundamental tool used in verification of gradients, which is the\n",
    "*second order Taylor remainder convergence test*. Thus, let us consider the functional $I(u_0)$  and let $\\nabla_{u_0}$ be its gradient with respect to the control parameter $u_0$. Let $u$ be the solution of the forward problem, and let $\\delta u$ be a perturbation to $u$. Then the *Taylor remainder convergence test* is based on the expression: \n",
    "$$ \\left|I(u + \\epsilon \\delta u) - I(u) - \\nabla_{u_0} I \\cdot \\epsilon \\delta u\\right| \\rightarrow 0 \\quad \\mathrm{at} \\ O(\\epsilon^2).$$\n",
    "\n",
    "In the next sections, we present the results of the adjoint-based sensitivity computation using the `checkpoint_schedules` package and the *Taylor remainder convergence test*. It is expected the convergence rate of the *Taylor remainder convergence test* is approximately $2$ in every checkpointing approach.\n",
    "\n",
    "Below, we define the `model` dictionary containing the parameters required for the forward and adjoint solvers. The `model` dictionary is then passed to the `BurgersEquation` class. Additionally, we set up the 1D mesh and the initial condition for the forward Burgers' solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {\"lx\": 1,   # domain length\n",
    "         \"nx\": 100,  # number of nodes\n",
    "         \"dt\": 0.001,  # time step\n",
    "         \"nu\": 0.01,  # viscosity\n",
    "         \"max_n\": 1,  # total steps\n",
    "         \"chk_ram\": 50,  # number of checkpoints in RAM\n",
    "         \"chk_disk\": 50,  # number of checkpoints on disk\n",
    "        }\n",
    "mesh = np.linspace(0, model[\"lx\"], model[\"nx\"]) # create the spatial grid\n",
    "u0 = np.sin(np.pi*mesh) # initial condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taylor_remainder_test(gradient, J, u0, h):\n",
    "    epsilons = [0.00001 / 2**i for i in range(4)]\n",
    "    E = []\n",
    "    dJdm = np.dot(gradient, h)\n",
    "    for eps in epsilons:\n",
    "        up = u0 + eps*h\n",
    "        burgers = BurgersEquation(model, up, mesh)\n",
    "        burgers.forward(0, model[\"max_n\"])\n",
    "        Jp = burgers.functional()\n",
    "        E.append(abs(Jp - J - dJdm * eps))\n",
    "        print(\"(Jp - J)/eps: {}\".format((Jp - J) / eps), \"dJdm: {}\".format(dJdm))\n",
    "        print(\"Computed residuals: {}\".format(E))\n",
    "    return E, epsilons\n",
    "\n",
    "def convergence_rates(E_values, eps_values, show=True):\n",
    "    from numpy import log\n",
    "    r = []\n",
    "    for i in range(1, len(eps_values)):\n",
    "        r.append(log(E_values[i] / E_values[i - 1])\n",
    "                 / log(eps_values[i] / eps_values[i - 1]))\n",
    "    if show:\n",
    "        print(\"Computed convergence rates: {}\".format(r))\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by computing the gradient without any checkpointing. Then, we compute the gradient using the `SingleMemoryStorageSchedule`, `SingleMemoryStorageSchedule`, `Revolve` and `HRevolve` checkpointing schedules. We then compare the gradients obtained from each checkpointing schedule to the gradient obtained without checkpointing. We expect the gradients to be the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(Jp - J)/eps: 30.899818375118567 dJdm: 30.899673112159043\n",
      "Computed residuals: [1.4526295952785664e-09]\n",
      "(Jp - J)/eps: 30.89974574379539 dJdm: 30.899673112159043\n",
      "Computed residuals: [1.4526295952785664e-09, 3.631581817491354e-10]\n",
      "(Jp - J)/eps: 30.89970942653508 dJdm: 30.899673112159043\n",
      "Computed residuals: [1.4526295952785664e-09, 3.631581817491354e-10, 9.078594009914211e-11]\n",
      "(Jp - J)/eps: 30.899691273589266 dJdm: 30.899673112159043\n",
      "Computed residuals: [1.4526295952785664e-09, 3.631581817491354e-10, 9.078594009914211e-11, 2.2701787783072254e-11]\n",
      "Computed convergence rates: [1.9999968897032412, 2.000057291912982, 1.9996629711817249]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.9999968897032412, 2.000057291912982, 1.9996629711817249]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "burger = BurgersEquation(model, u0, mesh)  # Create the burger's equation object.\n",
    "burger.forward(0, model[\"max_n\"], storage=StorageType.WORK, write_adj_deps=True)  # Solve the forward problem.\n",
    "burger.adjoint(0, model[\"max_n\"], clear_adj_deps=True)  # Solve the adjoint problem.\n",
    "base_gradient = burger.gradient()  # Compute the gradient.\n",
    "h = np.random.rand(model[\"nx\"])  # Random perturbation.\n",
    "E, epsilon = taylor_remainder_test(burger.gradient(), burger.functional(), u0, h)  # Taylor remainder test.\n",
    "convergence_rates(E, epsilon)  # Compute the convergence rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute the adjoint-based gradient using `SingleMemoryStorageSchedule` checkpointing approach. The `SingleMemoryStorageSchedule` stores the forward data of each time-step in working memory. As explained in the [notebook with illustrative example](https://nbviewer.org/github/firedrakeproject/checkpoint_schedules/blob/main/docs/notebooks/tutorial.ipynb), this schedule does not require the maximal step (`model[\"max_n\"]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(Jp - J)/eps: 30.899818375118567 dJdm: 30.899673112159043\n",
      "Computed residuals: [1.4526295952785664e-09]\n",
      "(Jp - J)/eps: 30.89974574379539 dJdm: 30.899673112159043\n",
      "Computed residuals: [1.4526295952785664e-09, 3.631581817491354e-10]\n",
      "(Jp - J)/eps: 30.89970942653508 dJdm: 30.899673112159043\n",
      "Computed residuals: [1.4526295952785664e-09, 3.631581817491354e-10, 9.078594009914211e-11]\n",
      "(Jp - J)/eps: 30.899691273589266 dJdm: 30.899673112159043\n",
      "Computed residuals: [1.4526295952785664e-09, 3.631581817491354e-10, 9.078594009914211e-11, 2.2701787783072254e-11]\n",
      "Computed convergence rates: [1.9999968897032412, 2.000057291912982, 1.9996629711817249]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.9999968897032412, 2.000057291912982, 1.9996629711817249]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schedule = SingleMemoryStorageSchedule()  # Create the checkpointing schedule.\n",
    "burger = BurgersEquation(model, u0, mesh)  # Create the burger's equation object.\n",
    "manager = CheckpointingManager(schedule, burger)  # Create the checkpointing manager.\n",
    "manager.execute()  # execute the checkpointing schedule using `SingleMemoryStorageSchedule` schedule.\n",
    "E, epsilon = taylor_remainder_test(burger.gradient(), burger.functional(), u0, h)  # Taylor remainder test.\n",
    "convergence_rates(E, epsilon)  # Compute the convergence rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following examples use `Revolve` [3] and `HRevolve` schedule [4]. The `Revolve` algorithm requires the definition of the maximal step `model[\"max_n\"]` before the execution of the forward solver, and also the specification of the number of checkpoints stored in memory (`model[\"max_n\"]`). Whereas `Revolve` allows only the storage in memory, the `Hrevolve` algorithm allows both disk and memory storage. The argumentos `model[\"max_n\"]`, `model[\"chk_ram\"]` and `model[\"chk_dins\"]` are required to define the `HRevolve` schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(Jp - J)/eps: 30.899818375118567 dJdm: 30.899673112159043\n",
      "Computed residuals: [1.4526295952785664e-09]\n",
      "(Jp - J)/eps: 30.89974574379539 dJdm: 30.899673112159043\n",
      "Computed residuals: [1.4526295952785664e-09, 3.631581817491354e-10]\n",
      "(Jp - J)/eps: 30.89970942653508 dJdm: 30.899673112159043\n",
      "Computed residuals: [1.4526295952785664e-09, 3.631581817491354e-10, 9.078594009914211e-11]\n",
      "(Jp - J)/eps: 30.899691273589266 dJdm: 30.899673112159043\n",
      "Computed residuals: [1.4526295952785664e-09, 3.631581817491354e-10, 9.078594009914211e-11, 2.2701787783072254e-11]\n",
      "Computed convergence rates: [1.9999968897032412, 2.000057291912982, 1.9996629711817249]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.9999968897032412, 2.000057291912982, 1.9996629711817249]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "burger = BurgersEquation(model, u0, mesh) # create the burger's equation object\n",
    "schedule = Revolve(model[\"max_n\"], model[\"chk_ram\"]) # create the checkpointing schedule\n",
    "manager = CheckpointingManager(schedule, burger)  # create the checkpointing manager\n",
    "manager.execute()  # execute the forward and adjoint solvers using checkpointing\n",
    "E, epsilon = taylor_remainder_test(burger.gradient(), burger.functional(), u0, h)  # Taylor remainder test.\n",
    "convergence_rates(E, epsilon)  # Compute the convergence rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[188], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m burger \u001b[38;5;241m=\u001b[39m BurgersEquation(model, u0, mesh) \u001b[38;5;66;03m# create the burger's equation object\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m schedule \u001b[38;5;241m=\u001b[39m \u001b[43mHRevolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_n\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchk_ram\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchk_disk\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# create the checkpointing schedule\u001b[39;00m\n\u001b[1;32m      3\u001b[0m manager \u001b[38;5;241m=\u001b[39m CheckpointingManager(schedule, burger)  \u001b[38;5;66;03m# create the checkpointing manager\u001b[39;00m\n\u001b[1;32m      4\u001b[0m manager\u001b[38;5;241m.\u001b[39mexecute()  \u001b[38;5;66;03m# execute the forward and adjoint solvers using checkpointing\u001b[39;00m\n",
      "File \u001b[0;32m~/work/my_venv/checkpoint_schedules/checkpoint_schedules/hrevolve.py:228\u001b[0m, in \u001b[0;36mHRevolve.__init__\u001b[0;34m(self, max_n, snapshots_in_ram, snapshots_on_disk, uf, ub, wd, rd)\u001b[0m\n\u001b[1;32m    226\u001b[0m wc \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, wd]\n\u001b[1;32m    227\u001b[0m rc \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, rd]\n\u001b[0;32m--> 228\u001b[0m schedule \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mhrevolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_n\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcvec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mub\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(max_n, snapshots_in_ram, snapshots_on_disk, schedule)\n",
      "File \u001b[0;32m~/work/my_venv/checkpoint_schedules/checkpoint_schedules/hrevolve_sequences/hrevolve.py:261\u001b[0m, in \u001b[0;36mhrevolve\u001b[0;34m(l, cvect, wvect, rvect, fwd_cost, bwd_cost)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"H-Revolve algorithm.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;124;03m    H-Revolve schedules.\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    260\u001b[0m params \u001b[38;5;241m=\u001b[39m revolver_parameters(wvect, rvect, fwd_cost, bwd_cost)\n\u001b[0;32m--> 261\u001b[0m h_rev \u001b[38;5;241m=\u001b[39m \u001b[43mhrevolve_recurse\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcvect\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcvect\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcvect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwvect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mhoptp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhopt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m h_rev\n",
      "File \u001b[0;32m~/work/my_venv/checkpoint_schedules/checkpoint_schedules/hrevolve_sequences/hrevolve.py:298\u001b[0m, in \u001b[0;36mhrevolve_recurse\u001b[0;34m(l, K, cmem, cvect, wvect, rvect, hoptp, hopt, **params)\u001b[0m\n\u001b[1;32m    296\u001b[0m ub \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mub\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (hoptp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (hopt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 298\u001b[0m     (hoptp, hopt) \u001b[38;5;241m=\u001b[39m \u001b[43mget_hopt_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcvect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwvect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mub\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m sequence \u001b[38;5;241m=\u001b[39m Sequence(Function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHRevolve\u001b[39m\u001b[38;5;124m\"\u001b[39m, l, [K, cmem]),\n\u001b[1;32m    300\u001b[0m                     levels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(cvect), concat\u001b[38;5;241m=\u001b[39mparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcat\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    301\u001b[0m operation \u001b[38;5;241m=\u001b[39m partial(Op, params\u001b[38;5;241m=\u001b[39mparameters)\n",
      "File \u001b[0;32m~/work/my_venv/checkpoint_schedules/checkpoint_schedules/hrevolve_sequences/hrevolve.py:64\u001b[0m, in \u001b[0;36mget_hopt_table\u001b[0;34m(lmax, cvect, wvect, rvect, ub, uf)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (k \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     63\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m         \u001b[43moptp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m[m] \u001b[38;5;241m=\u001b[39m uf \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m ub \u001b[38;5;241m+\u001b[39m rvect[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     65\u001b[0m         opt[k][\u001b[38;5;241m1\u001b[39m][m] \u001b[38;5;241m=\u001b[39m wvect[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m optp[k][\u001b[38;5;241m1\u001b[39m][m]\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Fill K = 0\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "burger = BurgersEquation(model, u0, mesh) # create the burger's equation object\n",
    "schedule = HRevolve(model[\"max_n\"], model[\"chk_ram\"], model[\"chk_disk\"]) # create the checkpointing schedule\n",
    "manager = CheckpointingManager(schedule, burger)  # create the checkpointing manager\n",
    "manager.execute()  # execute the forward and adjoint solvers using checkpointing\n",
    "E, epsilon = taylor_remainder_test(burger.gradient(), burger.functional(), u0, h)  # Taylor remainder test.\n",
    "convergence_rates(E, epsilon)  # Compute the convergence rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to explore alternative schedules provided by the `checkpoint_schedules` package. Our [notebook with illustrative example](https://nbviewer.org/github/firedrakeproject/checkpoint_schedules/blob/main/docs/notebooks/tutorial.ipynb) offers a demonstration of their usage. You need follow the steps outlined in the above code: instantiate a `BurgersEquation` object, define the `schedule`, create a `CheckpointingManager` object, and then execute the solvers using the `execute` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. Maddison, James R., Daniel N. Goldberg, and Benjamin D. Goddard. \"Automated calculation of higher order partial differential equation constrained derivative information.\" SIAM Journal on Scientific Computing 41.5 (2019): C417-C445. https://doi.org/10.1137/18M120946.\n",
    "2. Mitusch, Sebastian Kenji. An algorithmic differentiation tool for FEniCS. MS thesis. 2018.\n",
    "3. Griewank, A., & Walther, A. (2000). Algorithm 799: revolve: an implementation of checkpointing for the reverse or adjoint mode of computational differentiation. ACM Transactions on Mathematical Software (TOMS), 26(1), 19-45., doi: https://doi.org/10.1145/347837.347846.\n",
    "4. Herrmann, J. and Pallez (Aupy), G. (2020). H-Revolve: a framework for adjoint computation on synchronous hierarchical platforms. ACM Transactions on Mathematical Software (TOMS), 46(2), 1-25. DOI: https://doi.org/10.1145/3378672.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
