{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *checkpoint_schedule* application: adjoint-based gradient with Burger's equation\n",
    "This notebook aims to present the usage of the `checkpoint_schedules` package in an adjoint-based sensitivity problem. We begin by defining the adjoint-based gradient problem\n",
    "\n",
    "### Defining the application\n",
    "Let us consider a one-dimensional (1D) problem where it aims to compute the gradient/sensitivity of an objective functional $I$  with respect to a control parameter. In the current case, consider the initial condition $u_0$ as the control parameter, and the objective functional given by:\n",
    "$$\n",
    "I(u) = \\int_{\\Omega} \\frac{1}{2} u(x, \\tau)u(x, \\tau) \\, d x.\n",
    "\\tag{1}\n",
    "$$\n",
    "The objective functional $I(u)$ is the kinetic energy, the velocity variable $u = u(x, t)$ is governed by the 1D viscous Burgers equation, a nonlinear equation for the advection and diffusion on momentum:\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} - \\nu \\frac{\\partial^2 u}{\\partial x^2} = 0\n",
    "\\tag{2},\n",
    "$$\n",
    "where $x \\in [0, L]$ is the space variable and $t \\in \\mathbb{R}^{+}$ represents the time variable. The boundary condition is $u(0, t) = u(L, t) = 0$, where $L$ represents the lenght of the 1D domain. The initial condition is here given by $u(0, t) = u_0 = \\sin(\\pi x)$.\n",
    "\n",
    "In this current case, we aims to measure the sensitivity of the objective functional $I(u)$ with respect to the initial condition $u_0$ using the adjoint-based gradient method. Therefore, we need initially to define the adjoint-based gradient expression that is here set by using the continuous formulation, which means the sensitivity is obtained from the continuous forward PDE (Partial Differential Equation). Thus, in this case, let us consider an augmented functional $J$ given by:\n",
    "$$J(u, \\lambda, u_0, \\alpha) = I(u) - \\int_{0}^{\\tau} \\int_{\\Omega}  \\lambda \\left( \\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} - \\nu \\frac{\\partial^2 u}{\\partial x^2} \\right) u^{\\dagger} \\, d x \\, d t, - \\alpha \\int_{\\Omega} \\alpha u^{\\dagger}(x, 0) \\, d x, $$\n",
    "where $\\alpha$ and $\\lambda$ are the Lagrange multipliers also referred to as the adjoint variables. The adjoint-based gradient expression is obtained by applying the differentiation of the augmented functional $J$ with respect to the initial condition $u_0$, which results in:\n",
    "$$ \\frac{\\partial J}{\\partial u_0} = \\int_{\\Omega} (u(x, \\tau) - u_0(x)) \\, d x - \\alpha \\int_{\\Omega} u^{\\dagger}(x, 0) \\, d x. $$\n",
    "\n",
    "To compute the adjoint-based gradient, we need to compute the adjoint variable $\\lambda$, which is obtained by solving an adjoint equation. The adjoint equation is obtained by applying the differentiation of the augmented functional $J$ with respect to the velocity variable $u$, which results in:\n",
    "$$\n",
    "-\\frac{\\partial \\lambda}{\\partial t} + \\lambda \\frac{\\partial u}{\\partial x} - u \\frac{\\partial \\lambda}{\\partial x} - \\nu \\frac{\\partial^2 \\lambda}{\\partial x^2} = 0,\n",
    "\\tag{4}\n",
    "$$\n",
    "satisfying the boundary condition $\\lambda (0, t) = \\lambda(L, t) = 0$. In this case, the initial condition is $\\lambda (x, \\tau) = u(x, \\tau)$.\n",
    "\n",
    "Compute the adjoint-based gradient requires storing the forward solution for each time-step, since the adjoint equation depends on the forward solution as seen in adjoint equation (4). Additionally, the gradient expression (3) is a function of $\\lambda (0, t)$, which is the final adjoint time 0. \n",
    "\n",
    "#### Burger's equation discretisation\n",
    "Both the forward and adjoint systems are discretised using the Finite Element Method (FEM), employing a discretisation methodology detailed in [1]. This methodology uses the Galerkin Continuous method with linear trial basis functions to obtain an approximate solution. The backward finite difference method is employed to discretise the equations in time.\n",
    "\n",
    "#### Coding\n",
    "\n",
    "As the previous example, we use the `CheckpointingManager` class intending to manage the execution of forward and adjoint models using a checkpointing schedule. The `CheckpointingManager` has the method `execute` that executes every single action given the checkpointing schedule by iterating over the elements of the `cp_schedule`. Inside the `execute` method, we have the `action` function, which is a single-dispatch generic function. The `action` function is overloaded by the `action_forward`, `action_adjoint`, `action_copy`, `action_move`, `action_end_forward` and `action_end_reverse` according to the actions reached by the iterations over the elements of the `cp_schedule`, e.g., if the action is `Forward`, then the `action_forward` function is called and inside this function, we can implement the necessary code to step the forward model. In this current example, we execute the forward solver by calling `self.equation.forward`. Notice that the `self.equation` is an attribute of `CheckpointingManager`. Thus, this attribute would have a `forward` method that executes the forward solver, and an `adjoint` method that executes the adjoint solver. In addition, in `action_copy` and `action_move`, we add the `self.equation.copy_data`, in `action_end_forward` the `self.equation.adjoint_initial_condition()`. Therefore, the `self.equation` object would provide these required methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools, sys\n",
    "from checkpoint_schedules import *\n",
    "\n",
    "class CheckpointingManager:\n",
    "    \"\"\"Manage the forward and backward solvers.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    max_n : int\n",
    "        Total steps used to execute the solvers.\n",
    "    save_ram : int\n",
    "        Number of checkpoint that will be stored in RAM.\n",
    "    save_disk : int\n",
    "        Number of checkpoint that will be stored on disk.\n",
    "    forward_solver : object\n",
    "        The equation to solve.\n",
    "    forward_data : object\n",
    "        The data used by the forward solver.\n",
    "    \"\"\"\n",
    "    def __init__(self, schedule, equation):\n",
    "        self.max_n = sys.maxsize\n",
    "        self.equation = equation\n",
    "        self.reverse_step = 0\n",
    "        self._schedule = schedule\n",
    "        \n",
    "    def execute(self):\n",
    "        \"\"\"Execute forward and adjoint using checkpointing.\n",
    "        \"\"\"\n",
    "        @functools.singledispatch\n",
    "        def action(cp_action):\n",
    "            raise TypeError(\"Unexpected action\")\n",
    "\n",
    "        @action.register(Forward)\n",
    "        def action_forward(cp_action):\n",
    "            n1 = cp_action.n1\n",
    "            if (\n",
    "                isinstance(self._schedule, SingleMemoryStorageSchedule) \n",
    "                or isinstance(self._schedule, SingleDiskStorageSchedule)\n",
    "            ): \n",
    "                self.equation.forward(cp_action.n0, n1, storage=cp_action.storage,\n",
    "                                      single_storage=True, write_adj_deps=cp_action.write_adj_deps)\n",
    "            else:    \n",
    "                self.equation.forward(cp_action.n0, n1, storage=cp_action.storage,\n",
    "                                      write_adj_deps=cp_action.write_adj_deps, write_ics=cp_action.write_ics)\n",
    "            if n1 > self.equation.model[\"max_n\"]:\n",
    "                n1 = min(n1, self.equation.model[\"max_n\"])\n",
    "                self._schedule.finalize(n1)\n",
    "            \n",
    "\n",
    "        @action.register(Reverse)\n",
    "        def action_reverse(cp_action):\n",
    "            if self.reverse_step == 0:\n",
    "                self.equation.adjoint_initial_condition()\n",
    "            self.equation.adjoint(cp_action.n0, cp_action.n1, cp_action.clear_adj_deps)\n",
    "            self.reverse_step += cp_action.n1 - cp_action.n0\n",
    "            \n",
    "        @action.register(Copy)\n",
    "        def action_copy(cp_action):\n",
    "            self.equation.copy_data(cp_action.n, cp_action.from_storage, cp_action.to_storage)\n",
    "\n",
    "        @action.register(Move)\n",
    "        def action_move(cp_action):\n",
    "            self.equation.copy_data(cp_action.n, cp_action.from_storage, cp_action.to_storage, move=True)\n",
    "            \n",
    "        @action.register(EndForward)\n",
    "        def action_end_forward(cp_action):\n",
    "            if self._schedule.max_n is None:\n",
    "                self._schedule._max_n = self.max_n\n",
    "            assert self.reverse_step == 0\n",
    "            \n",
    "        @action.register(EndReverse)\n",
    "        def action_end_reverse(cp_action):\n",
    "            if self._schedule.max_n != self.reverse_step:\n",
    "                raise ValueError(\"The number of steps in the reverse phase\"\n",
    "                                 \"is different from the number of steps in the\"\n",
    "                                 \"forward phase.\")\n",
    "            \n",
    "        self.reverse_step = 0\n",
    "        for _, cp_action in enumerate(self._schedule):\n",
    "            action(cp_action)\n",
    "            if isinstance(cp_action, EndReverse):\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coding\n",
    "\n",
    "As we sad, the `CheckpointingManager` class has the `execute` method that executes every single action given the checkpointing schedule by iterating over the elements of the `cp_schedule`. To do so, we need to provide the equation. Here, we are handling with a adjoint-based sensitivity problem with Burger's equation. Thus, we need to provide the `BurgersEquation` class. The `BurgersEquation` class has the `forward` and `adjoint` methods that execute the forward and adjoint solvers, respectively. In addition, the `BurgersEquation` class has the `copy_data` and `adjoint_initial_condition` methods that copy the data from the forward solver to the adjoint solver and set the adjoint initial condition, respectively. The code is an implmentation  of the `BurgersEquation` with the methods required by solving the adjoint-based sensitivity problem using the checkpointing manager class `CheckpointingManager`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from enum import Enum\n",
    "import os\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy.sparse import lil_matrix\n",
    "from scipy.optimize import newton\n",
    "\n",
    "\n",
    "class BurgersEquation:\n",
    "    \"\"\"This class is capable to solve the time-dependent forward \n",
    "    and adjoint burger's equation.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    model : dict\n",
    "        The model parameters.\n",
    "    checkpoints : str, optional\n",
    "        Checkpointing method. The default is \"trivial\", which means that\n",
    "        the forward restart data and the adjoint dependency is stored every time, \n",
    "    snapshots : dict\n",
    "        Storage of the forward restart data.\n",
    "    forward_work_memory : dict\n",
    "        Store the forward restart data in StorageType.WORK.\n",
    "    adjoint_work_memory : dict\n",
    "        Store the adjoint restart data in StorageType.WORK.\n",
    "    adj_deps : dict\n",
    "        Store the adjoint dependency data in StorageType.WORK.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, model, forward_init_condition):\n",
    "        self.model = model\n",
    "        self.snapshots = {StorageType.RAM: {}, StorageType.DISK: {}}\n",
    "        self.forward_work_memory = {StorageType.WORK: {}}\n",
    "        self.forward_work_memory[StorageType.WORK][0] = forward_init_condition\n",
    "        self.adjoint_work_memory = {StorageType.WORK: {}}\n",
    "            \n",
    "    def forward(\n",
    "            self, n0, n1, storage=None, write_adj_deps=False,\n",
    "            write_ics=False, single_storage=False\n",
    "    ):\n",
    "        \"\"\"Solve the non-linear forward burger's equation in time.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n0 : int\n",
    "            Initial time step.\n",
    "        n1 : int\n",
    "            Final time step.\n",
    "        storage : StorageType, optional\n",
    "            The storage type, which can be StorageType.RAM, StorageType.DISK,\n",
    "            StorageType.WORK, or StorageType.NONE.\n",
    "        write_adj_deps : bool, optional\n",
    "            Whether the adjoint dependency data will be stored.\n",
    "        write_ics : bool, optional\n",
    "            Whether the forward restart data will be stored.\n",
    "        single_storage : bool, optional\n",
    "            This parameter is used to indicated whether a checkpointing schedule\n",
    "            is single storage or not. Single storage means that no checkpointing\n",
    "            algorithm (eg, `Revolve`, `HRevole`) is employed. \n",
    "        \"\"\"\n",
    "        # Get the model parameters\n",
    "        dx = self.model[\"lx\"] / self.model[\"nx\"]  # grid spacing\n",
    "        nx = self.model[\"nx\"]  # number of grid points\n",
    "        dt = self.model[\"dt\"]  # time step\n",
    "        nu = self.model[\"nu\"]  # viscosity\n",
    "        b = nu / (dx * dx)\n",
    "\n",
    "        def assemble_matrix_system(u_new):\n",
    "            \"\"\"This function assembles the matrix system.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            u_new : array\n",
    "                Forward solution at the n + 1 time step.\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            A , B : scipy.sparse.lil_matrix\n",
    "                Matrix systems.\n",
    "            \"\"\"\n",
    "            A = lil_matrix((nx, nx))\n",
    "            B = lil_matrix((nx, nx))  # mass matrix\n",
    "            B[0, 0] = - 1 / 3\n",
    "            B[0, 1] = - 1 / 6\n",
    "            B[nx - 1, nx - 1] = - 1 / 3\n",
    "            B[nx - 1, nx - 2] = - 1 / 6\n",
    "            A[0, 0] = 1 / 3 - dt * (1/2*u_new[0] / dx + b)\n",
    "            A[0, 1] = 1 / 6 + dt * (1 / 2 * u_new[0] / dx - b)\n",
    "            A[nx - 1, nx - 1] = 1 / 3 - dt * (- u_new[nx - 1] / dx + b)\n",
    "            A[nx - 1, nx - 2] = 1 / 6 + dt * (1 / 2 * u_new[nx - 2] / dx - b)\n",
    "\n",
    "            for i in range(1, nx - 1):\n",
    "                B[i, i] = - 2 / 3\n",
    "                B[i, i + 1] = B[i, i - 1] = - 1 / 6\n",
    "                A[i, i - 1] = 1 / 6 - dt * (1 / 2 * u_new[i - 1] / dx + b)\n",
    "                A[i, i] = 2 / 3 + dt * (1 / 2 * (u_new[i - 1] - u_new[i]) / dx + 2 * b)\n",
    "                A[i, i + 1] = 1 / 6 + dt * (1 / 2 * u_new[i] / dx - b)\n",
    "\n",
    "            return A, B\n",
    "\n",
    "        def non_linear(A, B, u_new, u):\n",
    "            u[0] = u[nx - 1] = 0\n",
    "            F = A * u_new + B * u\n",
    "            return F\n",
    "\n",
    "        # Get the initial condition\n",
    "        u = self.forward_work_memory[StorageType.WORK][n0]\n",
    "        if not single_storage:\n",
    "            del self.forward_work_memory[StorageType.WORK][n0]\n",
    "        u_new = u.copy()\n",
    "        n1 = min(n1, self.model[\"max_n\"])\n",
    "        step = n0\n",
    "        while step < n1:\n",
    "            if ((write_ics and step == n0)\n",
    "                or (write_adj_deps and storage != StorageType.WORK)):\n",
    "                self._store_data(u, step, storage,\n",
    "                                write_adj_deps=write_adj_deps,\n",
    "                                write_ics=write_ics)\n",
    "            A, B = assemble_matrix_system(u_new)\n",
    "            u_new = newton(lambda u_new: non_linear(A, B, u_new, u), u)\n",
    "            u = u_new.copy()\n",
    "            if single_storage and storage == StorageType.WORK:\n",
    "                self.forward_work_memory[StorageType.WORK][step] = u_new\n",
    "            step += 1\n",
    "        self.forward_work_memory[StorageType.WORK][step] = u_new\n",
    "\n",
    "    def adjoint(self, n0, n1, clear_adj_deps):\n",
    "        \"\"\"Execute the adjoint equation in time.\n",
    "\n",
    "        Parameters\n",
    "        ---------\n",
    "        n0 : int\n",
    "            Initial time step.\n",
    "        n1 : int\n",
    "            Final time step.\n",
    "        clear_adj_deps : bool\n",
    "            If `True`, the adjoint dependency data will be cleared.\n",
    "\n",
    "        \"\"\"\n",
    "        dx = self.model[\"lx\"] / self.model[\"nx\"]\n",
    "        nx = self.model[\"nx\"]\n",
    "        dt = self.model[\"dt\"]\n",
    "        nu = self.model[\"nu\"]\n",
    "        b = nu / (dx * dx)\n",
    "\n",
    "        def assemble_matrix_system(adjoint_dependency):\n",
    "            A = lil_matrix((nx, nx))\n",
    "            B = lil_matrix((nx, nx))\n",
    "            A[0, 0] = A[nx - 1, nx - 1] = 1 / 3\n",
    "            A[0, 1] = A[nx - 1, nx - 2] = 1 / 6\n",
    "            dfdx = (adjoint_dependency[1] - adjoint_dependency[0]) / dx\n",
    "            dfdxp = (adjoint_dependency[2] - adjoint_dependency[1]) / dx\n",
    "            B[0, 0] = 1 / 3 - dt * (adjoint_dependency[0] / dx - b - 1 / 3 * dfdx)\n",
    "            B[0, 1] = (1 / 6 + dt * (1 / 2 * adjoint_dependency[0] / dx + b - 1 / 6 * dfdxp))\n",
    "            dfdx = (adjoint_dependency[nx - 1] - adjoint_dependency[nx - 2]) / dx\n",
    "            B[nx - 1, nx - 1] = (1 / 3 + dt * (adjoint_dependency[nx - 1] / dx - b - 1 / 3 * dfdx))\n",
    "            B[nx - 1, nx - 2] = (1 / 6 + dt * (1 / 2 * u_new[nx - 2] / dx + b - 1 / 6 * dfdx))\n",
    "\n",
    "            for i in range(1, nx - 1):\n",
    "                v_m = adjoint_dependency[i] / dx\n",
    "                v_mm1 = adjoint_dependency[i - 1] / dx\n",
    "                deri = (adjoint_dependency[i] - adjoint_dependency[i - 1]) / dx\n",
    "                derip = (adjoint_dependency[i + 1] - adjoint_dependency[i]) / dx\n",
    "                A[i, i - 1] = A[i, i + 1] = 1 / 6\n",
    "                A[i, i] = 2 / 3\n",
    "                B[i, i] = 2 / 3 + dt * (1 / 2 * (v_mm1 - v_m) - 2 * b - 2 / 3 * (deri - derip))\n",
    "                B[i, i - 1] = 1/6 - dt * (1 / 2 * v_mm1 - b - 1 / 6 * deri)\n",
    "                B[i, i + 1] = 1/6 + dt*(1/2 * v_m + b - 1 / 6 * derip)\n",
    "            return A, B\n",
    "\n",
    "        u = self.adjoint_work_memory[StorageType.WORK][n1]\n",
    "        del self.adjoint_work_memory[StorageType.WORK][n1]\n",
    "        u_new = np.zeros(nx)\n",
    "        steps = n1 - n0\n",
    "        t = n1\n",
    "        for _ in range(steps):\n",
    "            u[0] = u[nx - 1] = 0\n",
    "            A, B = assemble_matrix_system(self.forward_work_memory[StorageType.WORK][t])\n",
    "            d = B.dot(u)\n",
    "            u_new = spsolve(A, d)\n",
    "            u = u_new.copy()\n",
    "            if clear_adj_deps:\n",
    "                del self.forward_work_memory[StorageType.WORK][t]\n",
    "            t -= 1\n",
    "        self.adjoint_work_memory[StorageType.WORK][n0] = u_new\n",
    "    \n",
    "    def compute_grad(self):\n",
    "        x = np.linspace(0, self.lx, self.nx)\n",
    "        sens = np.trapz(self.p[0]*1.01*np.sin(np.pi*x), x=x, dx=self.dx)\n",
    "        print(\"Sensitivity:\", sens)\n",
    "    \n",
    "    def _store_on_disk(self, data, step, adj_deps=False):\n",
    "        \"\"\"Store the forward data on disk.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : array\n",
    "            The forward data.\n",
    "        step : int\n",
    "            The time step.\n",
    "        adj_deps : bool, optional\n",
    "            If True, the data is stored in the adjoint dependencies folder.\n",
    "        \"\"\"\n",
    "        if adj_deps:\n",
    "            file_name = \"adj_deps/fwd_\"+ str(step) +\".npy\"\n",
    "            with open(file_name, \"wb\") as f:\n",
    "                np.save(f, data)\n",
    "        else:\n",
    "            file_name = \"fwd_data/fwd_\"+ str(step) +\".npy\"\n",
    "            with open(file_name, \"wb\") as f:\n",
    "                np.save(f, data)\n",
    "        self.snapshots[StorageType.DISK][step] = file_name\n",
    "\n",
    "    def copy_data(self, step, from_storage, to_storage, move=False):\n",
    "        \"\"\"Load the forward data from disk.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file_name : str\n",
    "            The file name.\n",
    "        step : int\n",
    "            The time step.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        data : array\n",
    "            The loaded data.\n",
    "        \"\"\"\n",
    "        if from_storage == StorageType.DISK:\n",
    "            file_name = self.snapshots[StorageType.DISK][step]\n",
    "            with open(file_name, \"rb\") as f:\n",
    "                if to_storage == StorageType.RAM:\n",
    "                    self.snapshots[StorageType.RAM][step] = np.load(f)\n",
    "                elif to_storage == StorageType.WORK:\n",
    "                    self.forward_work_memory[StorageType.WORK][step] = np.load(f)\n",
    "                if move:\n",
    "                    os.remove(file_name)\n",
    "        elif from_storage == StorageType.RAM:\n",
    "            self.forward_work_memory[StorageType.WORK][step] = self.snapshots[StorageType.RAM][step]\n",
    "            if move:\n",
    "                del self.snapshots[StorageType.RAM][step]\n",
    "\n",
    "    def _store_data(self, data, t, storage, write_adj_deps=False, write_ics=False):\n",
    "        if storage == StorageType.DISK:\n",
    "            if write_adj_deps:\n",
    "                self._store_on_disk(data, t, adj_deps=write_adj_deps)\n",
    "            if write_ics:\n",
    "                self._store_on_disk(data, t)\n",
    "        elif storage == StorageType.RAM:\n",
    "            self.snapshots[storage][t] = data\n",
    "\n",
    "\n",
    "    def adjoint_initial_condition(self):\n",
    "        \"\"\"Set the adjoint initial condition.\n",
    "        \"\"\"\n",
    "        self.adjoint_work_memory[StorageType.WORK][self.model[\"max_n\"]] = self.forward_work_memory[StorageType.WORK][self.model[\"max_n\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define the `model` dictionary that contains the parameters required for the forward and adjoint solvers. The `model` dictionary is passed to the `BurgersEquation` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {\"lx\": 1,   # lenght domain\n",
    "         \"nx\": 100, # number of nodes\n",
    "         \"dt\": 0.01, # time step\n",
    "         \"T\": 1, # final time\n",
    "         \"nu\": 0.01, # viscosity\n",
    "         \"max_n\": 10, # total steps\n",
    "         \"chk_ram\": 10, # number of checkpoints in RAM\n",
    "         \"chk_disk\": 0, # number of checkpoints on disk\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set the initial condition and the burger's equation object `burger`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, model[\"lx\"], model[\"nx\"]) # create the spatial grid\n",
    "u0 = np.sin(np.pi*x) # initial condition\n",
    "burger = BurgersEquation(model, u0) # create the burger's equation object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute a forward simulation\n",
    "`checkpoint_schedules` package provides a set of checkpointing schedules. The first employed here is `NoneCheckpointingSchedule` that executes only the forward solver without any storage in RAM or on disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ddolci/work/my_venv/lib/python3.11/site-packages/scipy/optimize/_zeros_py.py:482: RuntimeWarning: some failed to converge after 50 iterations\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "schedule = NoneCheckpointSchedule() # create the checkpointing schedule\n",
    "manager = CheckpointingManager(schedule, burger)  # create the checkpointing manager\n",
    "manager.execute()  # execute the forward and adjoint solvers using checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the adjoint-based gradient\n",
    "\n",
    "We finally present the adjoint-based gradient computation using differents checkpoints approaches available in the `checkpoint_schedules` package. The first one, it is `SingleMemoryStorageSchedule` for the cases where the user intend to store the forward data for all steps. As we explained in the illustration example, the `SingleMemoryStorageSchedule` schedule does not require the maximal step `model[\"max_n\"]`. Hence, we can define the final forward step while the forward solver is being executed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = SingleMemoryStorageSchedule()  # create the checkpointing schedule\n",
    "manager = CheckpointingManager(schedule, burger)  # create the checkpointing manager\n",
    "manager.execute()  # execute the forward and adjoint solvers using checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next example shows the usage of the `SingleDiskStorageSchedule` schedule. In this case, the forward data used in the adjoint compuations is stored on disk. The `SingleDiskStorageSchedule` schedule does not require the maximal step `model[\"max_n\"]` to be defined before the forward solver execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = SingleDiskStorageSchedule()  # create the checkpointing schedule\n",
    "manager = CheckpointingManager(schedule, burger)  # create the checkpointing manager\n",
    "manager.execute()  # execute the forward and adjoint solvers using checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the example above, we do not move any data from the disk to the work in memory, i.e., we copy the data from the disk and keep this data on disk. The next example shows the usage of the `SingleDiskStorageSchedule` schedule with the `mode_data=True` argument. In this case, the forward data used in the adjoint compuations stored on disk is moved to the work in memory, i.e., we copy the data from the disk and remove this data from the disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = SingleDiskStorageSchedule(move_data=True)  # create the checkpointing schedule\n",
    "manager = CheckpointingManager(schedule, burger)  # create the checkpointing manager\n",
    "manager.execute()  # execute the forward and adjoint solvers using checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we start to execute the adjoint-based gradient computation with the checkpointing algorithimic used to manage the memory usage. The first one is the `Revolve` schedule. The `Revolve` schedule requires the maximal step `model[\"max_n\"]` to be defined before the forward solver execution, and the number of checkpoints that we intend to store in RAM. \n",
    "\n",
    "In the following example, we define `model[\"chk_ram\"]` that holds the number of steps that the forward data is stored in  RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[\"chk_ram\"] = 2 # number of checkpoints in RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ddolci/work/my_venv/lib/python3.11/site-packages/scipy/optimize/_zeros_py.py:482: RuntimeWarning: some failed to converge after 50 iterations\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/Users/ddolci/work/my_venv/lib/python3.11/site-packages/scipy/sparse/linalg/_dsolve/linsolve.py:229: SparseEfficiencyWarning: spsolve requires A be CSC or CSR matrix format\n",
      "  warn('spsolve requires A be CSC or CSR matrix format',\n"
     ]
    }
   ],
   "source": [
    "burger = BurgersEquation(model, u0) # create the burger's equation object\n",
    "schedule = Revolve(model[\"max_n\"], model[\"chk_ram\"]) # create the checkpointing schedule\n",
    "manager = CheckpointingManager(schedule, burger)  # create the checkpointing manager\n",
    "manager.execute()  # execute the forward and adjoint solvers using checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we execute the adjoint-based gradient computation with the `DiskRevolve` schedule. We set the `model[\"chk_ram\"]` to 1, which that the forward data is stored in RAM for only one step and the remaining steps can be are stored on disk according the `DiskRevolve` algorithm. Thus, with the `DiskRevolve` schedule, we have a mixed storage approach, where the forward data is stored in RAM and on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ddolci/work/my_venv/lib/python3.11/site-packages/scipy/optimize/_zeros_py.py:482: RuntimeWarning: some failed to converge after 50 iterations\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/Users/ddolci/work/my_venv/lib/python3.11/site-packages/scipy/sparse/linalg/_dsolve/linsolve.py:229: SparseEfficiencyWarning: spsolve requires A be CSC or CSR matrix format\n",
      "  warn('spsolve requires A be CSC or CSR matrix format',\n"
     ]
    }
   ],
   "source": [
    "model[\"chk_ram\"] = 1 # number of checkpoints in RAM\n",
    "burger = BurgersEquation(model, u0) # create the burger's equation object\n",
    "schedule = DiskRevolve(model[\"max_n\"], model[\"chk_ram\"]) # create the checkpointing schedule\n",
    "manager = CheckpointingManager(schedule, burger)  # create the checkpointing manager\n",
    "manager.execute()  # execute the forward and adjoint solvers using checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows the usage of the `PeriodicDiskRevolve` schedule that also requires the maximal step `model[\"max_n\"]` to be defined before the forward solver execution. The `PeriodicDiskRevolve` schedule requires the number of checkpoints that we intend to store in RAM and computes the number of steps that the forward data is stored on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[\"chk_ram\"] = 1 # number of checkpoints in RAM\n",
    "burger = BurgersEquation(model, u0) # create the burger's equation object\n",
    "schedule = PeriodicDiskRevolve(model[\"max_n\"], model[\"chk_ram\"]) # create the checkpointing schedule\n",
    "manager = CheckpointingManager(schedule, burger)  # create the checkpointing manager\n",
    "manager.execute()  # execute the forward and adjoint solvers using checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `HRevolve` and `MultistageCheckpointSchedule` schedule are employed below, these checkpointing schedules require the maximal step `model[\"max_n\"]` to be defined before the forward solver execution, and the maximal number of checkpoints that we intend to store in RAM and on disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ddolci/work/my_venv/lib/python3.11/site-packages/scipy/optimize/_zeros_py.py:482: RuntimeWarning: some failed to converge after 50 iterations\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/Users/ddolci/work/my_venv/lib/python3.11/site-packages/scipy/sparse/linalg/_dsolve/linsolve.py:229: SparseEfficiencyWarning: spsolve requires A be CSC or CSR matrix format\n",
      "  warn('spsolve requires A be CSC or CSR matrix format',\n"
     ]
    }
   ],
   "source": [
    "model[\"chk_ram\"] = 1 # number of checkpoints in RAM\n",
    "model[\"chk_disk\"] = 1 # number of checkpoints on disk\n",
    "burger = BurgersEquation(model, u0) # create the burger's equation object\n",
    "schedule = HRevolve(model[\"max_n\"], model[\"chk_ram\"], model[\"chk_disk\"]) # create the checkpointing schedule\n",
    "manager = CheckpointingManager(schedule, burger)  # create the checkpointing manager\n",
    "manager.execute()  # execute the forward and adjoint solvers using checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[\"chk_ram\"] = 1 # number of checkpoints in RAM\n",
    "model[\"chk_disk\"] = 1 # number of checkpoints on disk\n",
    "burger = BurgersEquation(model, u0) # create the burger's equation object\n",
    "schedule = MultistageCheckpointSchedule(model[\"max_n\"], model[\"chk_ram\"], model[\"chk_disk\"]) # create the checkpointing schedule\n",
    "manager = CheckpointingManager(schedule, burger)  # create the checkpointing manager\n",
    "manager.execute()  # execute the forward and adjoint solvers using checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TwoLevelCheckpointSchedule` also employes a mixed storage in RAM and on disk. This schedule does not require the maximal step `model[\"max_n\"]` to be defined before the forward solver execution. `TwoLevelCheckpointSchedule` saves the forward data in RAM or on disk according to the `period` argument, e.g., if `period=2`, then the forward data is stored in RAM or on disk every two steps. In the reverse computation, which involves the the execution of the adjoint solver and the forward solver to compute the adjoint dependency, the  user can define the aditional foward restart data storage in RAM or on disk according. The aditional forward restart data storage is defined the second argument of the `TwoLevelCheckpointSchedule` schedule. In the example, below, we define the `model[\"chk_ram\"]` to 1. The storage type is defined by the `binomial_storage` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ddolci/work/my_venv/lib/python3.11/site-packages/scipy/optimize/_zeros_py.py:482: RuntimeWarning: some failed to converge after 50 iterations\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/Users/ddolci/work/my_venv/lib/python3.11/site-packages/scipy/sparse/linalg/_dsolve/linsolve.py:229: SparseEfficiencyWarning: spsolve requires A be CSC or CSR matrix format\n",
      "  warn('spsolve requires A be CSC or CSR matrix format',\n"
     ]
    }
   ],
   "source": [
    "model[\"chk_ram\"] = 1 # number of checkpoints in RAM\n",
    "model[\"chk_disk\"] = 1 # number of checkpoints on disk\n",
    "burger = BurgersEquation(model, u0) # create the burger's equation object\n",
    "period = 2\n",
    "schedule = TwoLevelCheckpointSchedule(period, model[\"chk_ram\"], binomial_storage=StorageType.RAM) # create the checkpointing schedule\n",
    "manager = CheckpointingManager(schedule, burger)  # create the checkpointing manager\n",
    "manager.execute()  # execute the forward and adjoint solvers using checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, we choose to store one additional forward restart data in RAM in the reverse computation with the storage type `binomial_storage=StorageType.RAM`. In the example below, we choose to store one additional forward restart data on disk in the reverse computation with the storage type `binomial_storage=StorageType.DISK`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[\"chk_ram\"] = 1 # number of checkpoints in RAM\n",
    "model[\"chk_disk\"] = 1 # number of checkpoints on disk\n",
    "burger = BurgersEquation(model, u0) # create the burger's equation object\n",
    "period = 2\n",
    "schedule = TwoLevelCheckpointSchedule(period, model[\"chk_disk\"], binomial_storage=StorageType.DISK) # create the checkpointing schedule\n",
    "manager = CheckpointingManager(schedule, burger)  # create the checkpointing manager\n",
    "manager.execute()  # execute the forward and adjoint solvers using checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that every result obtained with every single checkpointing schedules were the same. The difference between the results is the memory usage and the execution time intrisic to each checkpointing algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
