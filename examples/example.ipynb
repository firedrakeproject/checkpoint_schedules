{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "To exemplify the *checkpoint_schedules* usage in adjoint-based gradient problems, let us consider a class that illustrates the implementation of an adjoint-based gradient. Below is the `GradAdj` class, which includes the forward and backward methods to execute respectively the forward and adjoint systems. The `copy_fwd_data` function carries forward data copying from either RAM or disk. This data is then used as the initial condition in the forward solver restarting. The functions `store_ram` and `store_disk` are responsible for storing the forward data for restarting purposes. Additionally, the `store_adj_deps` function is responsible for storing the forward data required for the adjoint computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradAdj():\n",
    "    \"\"\"This class illustrates an adjoint-based gradient computation.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.snapshots = {'RAM': {}, 'DISK': {}}\n",
    "        self.adj_deps = {}\n",
    "        self.adj_tape = {}\n",
    "        self.fwd_tape = {0: 0} #illustrating an initial condition at step 0\n",
    "    \n",
    "  \n",
    "    def forward(self, n0, n1, write_ics=False, write_adj_deps=False, storage=None):\n",
    "        \"\"\"Execute a forward solver in time.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n0 : int\n",
    "            Initial step.\n",
    "        n1 : int\n",
    "            Final step.\n",
    "        write_ics : bool, optional\n",
    "            Write the forward data to disk or RAM. \n",
    "            This data will be used as the initial condition for the forward solver restarting.\n",
    "        write_adj_deps : bool, optional\n",
    "            Write the forward data to disk or RAM. This data is an adjoint dependence.\n",
    "        \"\"\"\n",
    "        data_n = self.fwd_tape[n0]\n",
    "        if write_ics:\n",
    "            if storage == 'RAM':\n",
    "                self.store_in_ram(data_n, n0)\n",
    "            elif storage == 'DISK':\n",
    "                self.store_on_disk(data_n, n0)\n",
    "        \n",
    "        steps = int(n1 - n0)\n",
    "        t = 0\n",
    "        while t < steps:\n",
    "            data_np1 = data_n + 1\n",
    "            data_n = data_np1\n",
    "            t += 1\n",
    "            \n",
    "        self.fwd_tape.clear()    \n",
    "        self.fwd_tape = {n1: data_np1}\n",
    "        \n",
    "        if write_adj_deps:\n",
    "            self.store_adj_deps(data_np1, n1)\n",
    "\n",
    "\n",
    "    def backward(self, n0, n1, clear_adj_deps):\n",
    "        \"\"\"Execute the adjoint solver in time.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n0 : int\n",
    "            Initial time step.\n",
    "        n1 : int\n",
    "            Final time step.\n",
    "        clear_adj_deps : bool\n",
    "            Clear the adjoint dependencies.\n",
    "        \"\"\"\n",
    "        bwd_data_n = self.adj_tape[n1]\n",
    "        steps = int(n1 - n0)\n",
    "        t = 0\n",
    "        while t < steps:\n",
    "            bwd_data_np1 = bwd_data_n - 1\n",
    "            bwd_data_n = bwd_data_np1\n",
    "            t += 1\n",
    "        self.adj_tape = {n0: bwd_data_n}\n",
    "        if clear_adj_deps:\n",
    "            self.adj_deps.clear()\n",
    "        \n",
    "\n",
    "    def copy_fwd_data(self, n, from_storage, delete):\n",
    "        \"\"\"Copy the forward data from RAM or disk to the forward tape.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n : int\n",
    "            Time step.\n",
    "        from_storage : str\n",
    "            Storage type.\n",
    "        delete : bool\n",
    "            Delete the forward data stored either in RAM or disk.\n",
    "        \"\"\"\n",
    "        if from_storage == 'DISK':\n",
    "            u0 = self.snapshots[from_storage][n]\n",
    "        else:\n",
    "            u0 = self.snapshots[from_storage][n]\n",
    "        \n",
    "        self.fwd_tape.clear()\n",
    "        self.fwd_tape = {n: u0}\n",
    "        \n",
    "        if delete:\n",
    "            del self.snapshots[from_storage][n]\n",
    "    \n",
    "\n",
    "    def store_in_ram(self, data, step):\n",
    "        \"\"\"Store the forward data in RAM.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : array\n",
    "            Forward data.\n",
    "        step : int\n",
    "            Time step.\n",
    "        \"\"\"\n",
    "        self.snapshots['RAM'][step] = data\n",
    "\n",
    "\n",
    "    def store_on_disk(self, data, step):\n",
    "        \"\"\"Store the forward data on disk.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : array\n",
    "            Forward data.\n",
    "        step : int\n",
    "            Time step.\n",
    "        \"\"\"\n",
    "        self.snapshots['DISK'][step] = data\n",
    "        \n",
    "    def store_adj_deps(self, data, n):\n",
    "        \"\"\"Store the adjoint dependencies.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : array\n",
    "            Adjoint dependencies.\n",
    "        n : int\n",
    "            Time step.\n",
    "        \"\"\"\n",
    "        self.adj_deps = {n: data}\n",
    "    \n",
    "    def adj_initcondition(self, data, n):\n",
    "        self.adj_tape = {n: data}\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using *checkpoint_schedules* package\n",
    "The *checkpoint_schedules* package offers a squedule of actions that enable the coordination of an adjoint-based gradient executions through a checkpoint strategy. The checkpoint schedule is built by the sequence of actions referred to as *Forward, EndForward, Reverse, Copy, EndReverse*. The actions provides functionalities such as storing the forward checkpoint data used to restart the forward solver, storing the forward checkpoint data for adjoint computations, and retrieving the stored data for both the forward solver restart and the adjoint computation. Additionally, *checkpoint_schedules* provides an iterator that convert revolver operations into the *checkpoint_schedules* format. \n",
    "\n",
    "In the following code, we have implemented the `CheckpointingManager` class, which allows the manegement of the forward and adjoint executions in time. With `CheckpointingManager.execute` method, we iterate over a sequence of actions given by the schedule `cp_schedule`. The actions are defined by using single-dispatch functions, where the `action` function is the generic function using the singledispatch decorator. Specific functions for different types of *checkpoint_schedules* actions are provided by using the register method of the base function `action`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checkpoint_schedules import Forward, EndForward, Reverse, Copy, EndReverse, StorageLocation\n",
    "import functools\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "class CheckpointingManager():\n",
    "    \"\"\"Manage the forward and adjoint solvers.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    adj_grad_problem : object\n",
    "        Adjoint-based gradient object.\n",
    "    save_ram : int\n",
    "        Number of checkpoint that will be stored in RAM.\n",
    "    save_disk : int\n",
    "        Number of checkpoint that will be stored on disk.\n",
    "    list_actions : list\n",
    "        Store the list of actions.\n",
    "    max_n : int\n",
    "        Total steps used to execute the solvers.\n",
    "    \"\"\"\n",
    "    def __init__(self, adj_grad_problem, max_n, save_ram, save_disk):\n",
    "        self.max_n = max_n\n",
    "        self.save_ram = save_ram\n",
    "        self.save_disk = save_disk\n",
    "        self.adj_grad_problem = adj_grad_problem\n",
    "        self.list_actions = []\n",
    "        \n",
    "    def execute(self, cp_schedule):\n",
    "        \"\"\"Execute forward and adjoint with a checkpointing strategy.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cp_schedule : object\n",
    "            Checkpointing schedule.\n",
    "        \"\"\"\n",
    "        @functools.singledispatch\n",
    "        def action(cp_action):\n",
    "            raise TypeError(\"Unexpected action\")\n",
    "\n",
    "        @action.register(Forward)\n",
    "        def action_forward(cp_action):\n",
    "            nonlocal model_n\n",
    "            print(Fore.BLUE + (\"|\" + \"--->\"*(cp_action.n1-cp_action.n0)).rjust(cp_action.n1*4))\n",
    "            self.adj_grad_problem.forward(cp_action.n0, cp_action.n1, \n",
    "                                          write_ics=cp_action.write_ics, \n",
    "                                          write_adj_deps=cp_action.write_adj_deps,\n",
    "                                          storage=cp_action.storage)\n",
    "\n",
    "            n1 = min(cp_action.n1, self.max_n)\n",
    "            model_n = n1\n",
    "            if cp_action.n1 == self.max_n:\n",
    "                cp_schedule.finalize(n1)\n",
    "\n",
    "        @action.register(Reverse)\n",
    "        def action_reverse(cp_action):\n",
    "            nonlocal model_r\n",
    "            print(Fore.RED + (\"<---\"*(cp_action.n1-cp_action.n0) + \"|\").rjust(cp_action.n1*4))\n",
    "            self.adj_grad_problem.backward(cp_action.n0, cp_action.n1, \n",
    "                                           cp_action.clear_adj_deps)\n",
    "            model_r += cp_action.n1 - cp_action.n0\n",
    "            \n",
    "        @action.register(Copy)\n",
    "        def action_copy(cp_action):\n",
    "            self.adj_grad_problem.copy_fwd_data(cp_action.n, \n",
    "                                                cp_action.from_storage, \n",
    "                                                cp_action.delete)\n",
    "    \n",
    "        @action.register(EndForward)\n",
    "        def action_end_forward(cp_action):\n",
    "            assert model_n == self.max_n\n",
    "            self.adj_grad_problem.adj_initcondition(model_n, model_n)\n",
    "            \n",
    "        @action.register(EndReverse)\n",
    "        def action_end_reverse(cp_action):\n",
    "            nonlocal model_r\n",
    "            assert model_r == self.max_n\n",
    "\n",
    "        model_n = 0\n",
    "        model_r = 0\n",
    "\n",
    "        storage_limits = {StorageLocation(0).name: self.save_ram, \n",
    "                          StorageLocation(1).name: self.save_disk}\n",
    "\n",
    "        count = 0\n",
    "        print(\"|---\"*(max_n) + \"|\")\n",
    "        while True:\n",
    "            print()\n",
    "            cp_action = next(cp_schedule)\n",
    "            action(cp_action)\n",
    "            self.list_actions.append([count, str(cp_action)])\n",
    "            count += 1\n",
    "            if isinstance(cp_action, EndReverse):  \n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let us define the adjoint-based gradient object. Also, set the total steps used in the computations, the number of steps that the checkpoint data is going to be saved in RAM and disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_adj = GradAdj() # Defining the adjoint-based gradient.\n",
    "max_n = 4 # Total number of time steps.\n",
    "save_ram = 1 # Number of steps to save i RAM.\n",
    "save_disk = 1 # Number of steps to save in disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us set the `CheckpointingManager` manage object, where one of the attributes is the adjoint-based gradient object, `grad_adj`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chk_manager = CheckpointingManager(grad_adj, max_n, save_ram, save_disk) # manager object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *checkpoint_schedules* package has already provided revolver algorithimics originally implemented by . However, using the API required to solver adjoint-based gradient problem that makes explicit the storing, copying and deleting of the data required for the forward solver restarting and for the use in the adjoint computation. For technical details involving the revolver algorithimics, please access the documentation. \n",
    "\n",
    "Below we choose to build the checkpoint schedule trhough on H-Revolve algorithmic, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checkpoint_schedules import HRevolve\n",
    "revolver = HRevolve(max_n, save_ram, snap_on_disk=save_disk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then set obtain the sequence by `revolver.sequence()` and execute the forward and adjoint solver with checkpointing methods with `chk_manager.execute(revolver)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|---|---|---|---|\n",
      "\n",
      "\u001b[34m|--->--->\n",
      "\n",
      "\u001b[34m       |--->\n",
      "\n",
      "\u001b[34m           |--->\n",
      "\n",
      "\n",
      "\u001b[31m           <---|\n",
      "\n",
      "\n",
      "\u001b[34m       |--->\n",
      "\n",
      "\u001b[31m       <---|\n",
      "\n",
      "\n",
      "\u001b[34m|--->\n",
      "\n",
      "\u001b[34m   |--->\n",
      "\n",
      "\u001b[31m   <---|\n",
      "\n",
      "\n",
      "\u001b[34m|--->\n",
      "\n",
      "\u001b[31m<---|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "revolver.sequence()\n",
    "chk_manager.execute(revolver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above illustrates how it works the forward and adjoint executions with the *checkpoint_schedules* package. The symbol `|` indicates the step that the solver initialise. Hence, \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from tabulate import tabulate\n",
    "print(Fore.BLACK + tabulate(chk_manager.list_actions, headers=[\"Action number\", \"checkpoint_schedules actions\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we presented the role of the *checkpoint_schedules* actions for some of the cases:\n",
    "\n",
    "* Action number 0: *Forward(0, 2, True, False, 'RAM')*:\n",
    "    - Execute the forward solver from step 0 to step 2.\n",
    "    - Write the forward data (*write_ics* is True) of step 0 to RAM (storage).\n",
    "    - The forward data is not stored for the adjoint computation (*write_adj_deps* is False).\n",
    "\n",
    "* Action number 2: *Forward(3, 4, False, True, 'RAM')*:\n",
    "    - Execute the forward solver from step 3 to step 4.\n",
    "    - Do not write the forward data (*write_ics* is False) of step 4.\n",
    "    - Store the forward data for the adjoint computation (*write_adj_deps* is *True*) in RAM (storage).\n",
    "\n",
    "* Action number 4: *Reverse(4, 3, True)*:\n",
    "    - Execute the adjoint solver from step 4 to step 3.\n",
    "    - Clear the adjoint dependencies (*clear_adj_deps* is True) used in the adjoint computation.\n",
    "\n",
    "* Action number 5: Copy(2, 'RAM', 'TAPE', True):\n",
    "    - Copy the forward data related to step 2 from RAM to TAPE.\n",
    "    - Delete the copied data from RAM (*delete* is *True*) as it is not needed anymore to restart the forward solver.\n",
    "\n",
    "* Action number 8: Copy(0, 'DISK', 'TAPE', True):\n",
    "    - Copy the forward data related to step 0 from DISK to TAPE.\n",
    "    - Do not delete the copied data from DISK (*delete* is *FALSE*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
